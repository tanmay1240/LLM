{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNjMj6m92X8W+FG4CIYOgsh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanmay1240/LLM/blob/main/Brain_Eyes_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECK_vIqnp8gq",
        "outputId": "ab574083-9854-4ca1-e122-d6b4e8542fa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.5)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade transformers accelerate torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from PIL import Image\n",
        "import re\n",
        "\n",
        "# =====================================================\n",
        "# 1. Load Vision Model (BLIP)\n",
        "# =====================================================\n",
        "captioner = pipeline(\n",
        "    \"image-to-text\",\n",
        "    model=\"Salesforce/blip-image-captioning-large\"\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 1.5 Load VQA Model (Emotion Probe)\n",
        "# =====================================================\n",
        "vqa = pipeline(\n",
        "    \"visual-question-answering\",\n",
        "    model=\"Salesforce/blip-vqa-base\"\n",
        ")\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# 2. Load LLM\n",
        "# =====================================================\n",
        "llm = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "    max_new_tokens=120,\n",
        "    temperature=0.1,\n",
        "    do_sample=False,\n",
        "    return_full_text=False\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 3. Load Image and Generate Caption\n",
        "# =====================================================\n",
        "image_path = input(\"Enter the image path of the image you want to analyze: \")\n",
        "\n",
        "try:\n",
        "    image = Image.open(image_path)\n",
        "except:\n",
        "    print(\"‚ùå Error: Could not open the image. Please check the path.\")\n",
        "    exit()\n",
        "\n",
        "caption_result = captioner(image)\n",
        "caption = caption_result[0][\"generated_text\"]\n",
        "\n",
        "print(\"\\nüîç Image Caption:\")\n",
        "print(caption)\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# -------------------------------\n",
        "# VQA Emotion Hint (Moved here after image is loaded)\n",
        "# -------------------------------\n",
        "vqa_result = vqa(\n",
        "    image=image,\n",
        "    question=\"What emotion is the person showing?\"\n",
        ")\n",
        "\n",
        "vqa_emotion = vqa_result[0][\"answer\"].lower()\n",
        "\n",
        "print(\"\\nüéØ VQA Emotion Hint:\")\n",
        "print(vqa_emotion)\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# 4. Build Prompt for LLM\n",
        "# =====================================================\n",
        "prompt = f\"\"\"\n",
        "You MUST return output in exactly this format:\n",
        "Human: Yes/No\n",
        "Emotion: one of [Happy, Sad, Angry, Neutral, Fear, Surprise, Not Applicable]\n",
        "Reasoning: one sentence explanation\n",
        "\n",
        "Rules:\n",
        "- Exactly ONE output block.\n",
        "- No repetition.\n",
        "- No additional examples.\n",
        "- Do NOT invent any story or context.\n",
        "- Do NOT add information not present in the description.\n",
        "- Do NOT infer emotion from pets, animals, environment, or objects.\n",
        "- Only use explicit emotional cues from the description.\n",
        "- Concerned, worried, nervous ‚Üí map to Fear.\n",
        "- If no clear emotion matches, use Neutral.\n",
        "- If no human is present, Emotion = Not Applicable.\n",
        "\n",
        "Description:\n",
        "{caption}\n",
        "\"\"\"\n",
        "\n",
        "# =====================================================\n",
        "# 5. Run LLM\n",
        "# =====================================================\n",
        "llm_result = llm(prompt)\n",
        "raw_output = llm_result[0][\"generated_text\"]\n",
        "\n",
        "# =====================================================\n",
        "# 6. Extract First Valid Answer Block\n",
        "# =====================================================\n",
        "pattern = r\"Human:\\s.*?\\nEmotion:\\s.*?\\nReasoning:.*?(?=\\nHuman:|\\Z)\"\n",
        "match = re.search(pattern, raw_output, re.DOTALL)\n",
        "\n",
        "if match:\n",
        "    answer = match.group(0).strip()\n",
        "else:\n",
        "    answer = raw_output.strip()\n",
        "\n",
        "# =====================================================\n",
        "# 7. Parse Fields\n",
        "# =====================================================\n",
        "human_match = re.search(r\"Human:\\s*(Yes|No)\", answer, re.IGNORECASE)\n",
        "emotion_match = re.search(r\"Emotion:\\s*([\\w\\s]+)\", answer, re.IGNORECASE)\n",
        "reasoning_match = re.search(r\"Reasoning:\\s*(.*)\", answer, re.DOTALL | re.IGNORECASE)\n",
        "\n",
        "human = human_match.group(1).capitalize() if human_match else \"Unknown\"\n",
        "emotion = emotion_match.group(1).strip().capitalize() if emotion_match else \"Unknown\"\n",
        "reasoning = reasoning_match.group(1).strip() if reasoning_match else \"\"\n",
        "\n",
        "# =====================================================\n",
        "# 8. Correct Human Detection Logic (Priority Based)\n",
        "# =====================================================\n",
        "caption_lower = caption.lower()\n",
        "\n",
        "human_keywords = [\n",
        "    \"person\", \"people\", \"man\", \"woman\",\n",
        "    \"boy\", \"girl\", \"child\", \"couple\",\n",
        "    \"human\", \"face\"\n",
        "]\n",
        "\n",
        "non_human_keywords = [\n",
        "    \"car\", \"vehicle\", \"building\", \"street\", \"road\",\n",
        "    \"house\", \"tree\", \"nature\", \"landscape\",\n",
        "    \"object\", \"sky\", \"cloud\", \"mountain\"\n",
        "]\n",
        "\n",
        "# Priority:\n",
        "# 1. If human keywords ‚Üí Human = Yes\n",
        "# 2. Else if non-human keywords ‚Üí Human = No\n",
        "# 3. Else ‚Üí Unknown\n",
        "if any(word in caption_lower for word in human_keywords):\n",
        "    human = \"Yes\"\n",
        "elif any(word in caption_lower for word in non_human_keywords):\n",
        "    human = \"No\"\n",
        "else:\n",
        "    human = \"Unknown\"\n",
        "\n",
        "# =====================================================\n",
        "# Hard Override for Non-Human Images\n",
        "# =====================================================\n",
        "if human == \"No\":\n",
        "    emotion = \"Not Applicable\"\n",
        "    reasoning = \"No humans are present in the image, so emotion classification is not applicable.\"\n",
        "\n",
        "# =====================================================\n",
        "# 8.5 Emotion Refinement Layer (Caption-Only + Priority)\n",
        "# =====================================================\n",
        "emotion_map = {\n",
        "    \"Angry\": [\n",
        "        \"angry\", \"anger\", \"furious\", \"annoyed\",\n",
        "        \"shouting\", \"yelling\", \"screaming\",\n",
        "        \"scowl\", \"scowling\", \"glare\", \"glaring\",\n",
        "        \"hostile\", \"aggressive\", \"rage\"\n",
        "    ],\n",
        "    \"Fear\": [\n",
        "        \"fear\", \"afraid\", \"scared\", \"worried\",\n",
        "        \"nervous\", \"anxious\", \"concerned\",\n",
        "        \"terrified\", \"panic\"\n",
        "    ],\n",
        "    \"Sad\": [\n",
        "        \"cry\", \"crying\", \"sad\", \"sadness\",\n",
        "        \"upset\", \"teary\", \"tear\", \"tears\", \"weeping\"\n",
        "    ],\n",
        "    \"Happy\": [\n",
        "        \"smile\", \"smiling\", \"laugh\", \"laughing\",\n",
        "        \"cheerful\", \"joyful\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Priority order: Angry > Fear > Sad > Happy\n",
        "emotion_priority = [\"Angry\", \"Fear\", \"Sad\", \"Happy\"]\n",
        "\n",
        "detected_emotion = None\n",
        "for emotion_label in emotion_priority:\n",
        "    keywords = emotion_map[emotion_label]\n",
        "    if any(word in caption_lower for word in keywords):\n",
        "        detected_emotion = emotion_label\n",
        "        break\n",
        "\n",
        "# =====================================================\n",
        "# Emotion Fusion Logic (Caption + VQA)\n",
        "# =====================================================\n",
        "\n",
        "valid_vqa_emotions = [\"happy\", \"sad\", \"angry\", \"fear\", \"scared\", \"neutral\"]\n",
        "\n",
        "if human == \"Yes\":\n",
        "    if detected_emotion:\n",
        "        emotion = detected_emotion\n",
        "        reasoning = (\n",
        "            f\"The description contains emotional cues such as {emotion.lower()} related expressions, \"\n",
        "            f\"so the emotion is classified as {emotion}.\"\n",
        "        )\n",
        "\n",
        "    elif vqa_emotion in valid_vqa_emotions:\n",
        "        # Normalize fear synonyms\n",
        "        if vqa_emotion == \"scared\":\n",
        "            emotion = \"Fear\"\n",
        "        else:\n",
        "            emotion = vqa_emotion.capitalize()\n",
        "\n",
        "        reasoning = (\n",
        "            f\"The visual question answering model suggests the emotion '{emotion}', \"\n",
        "            f\"which is used when no explicit emotional cues are found in the caption.\"\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        emotion = \"Neutral\"\n",
        "        reasoning = (\n",
        "            \"The description mentions a human, but neither the caption nor the visual question answering model \"\n",
        "            \"provides clear emotional cues. Therefore, the emotion is classified as Neutral.\"\n",
        "        )\n",
        "else:\n",
        "    emotion = \"Not Applicable\"\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# 9. Prevent Story Hallucination\n",
        "# =====================================================\n",
        "forbidden_words = [\"home\", \"work\", \"school\", \"office\", \"after\", \"day\"]\n",
        "\n",
        "if any(word in reasoning.lower() for word in forbidden_words):\n",
        "    reasoning = (\n",
        "        \"The description contains a human, but no contextual background or life situation \"\n",
        "        \"is provided. The reasoning must rely only on the visual description.\"\n",
        "    )\n",
        "\n",
        "# =====================================================\n",
        "# 10. Final Output\n",
        "# =====================================================\n",
        "final_output = f\"\"\"\n",
        "üß† Multimodal Reasoning Output\n",
        "-----------------------------\n",
        "Human: {human}\n",
        "Emotion: {emotion}\n",
        "Reasoning: {reasoning}\n",
        "\"\"\"\n",
        "\n",
        "print(final_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfPmS1iXqAiT",
        "outputId": "4b814f09-db3c-4dec-e973-e071feb92be9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Device set to use cpu\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the image path of the image you want to analyze: /content/smile.avif\n",
            "\n",
            "üîç Image Caption:\n",
            "smiling man with beard and green shirt looking at camera\n",
            "--------------------------------------------------\n",
            "\n",
            "üéØ VQA Emotion Hint:\n",
            "happiness\n",
            "\n",
            "üß† Multimodal Reasoning Output\n",
            "-----------------------------\n",
            "Human: Yes\n",
            "Emotion: Happy\n",
            "Reasoning: The description contains emotional cues such as happy related expressions, so the emotion is classified as Happy.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}