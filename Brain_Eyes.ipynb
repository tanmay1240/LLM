{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhqaLqpenDADEtmE9kxSVn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanmay1240/LLM/blob/main/Brain_Eyes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZV0pfsgkOwx"
      },
      "outputs": [],
      "source": [
        "pip install transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from PIL import Image\n",
        "import re\n",
        "\n",
        "# =====================================================\n",
        "# 1. Load Vision Model (BLIP)\n",
        "# =====================================================\n",
        "captioner = pipeline(\n",
        "    \"image-to-text\",\n",
        "    model=\"Salesforce/blip-image-captioning-large\"\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 2. Load LLM\n",
        "# =====================================================\n",
        "llm = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "    max_new_tokens=120,\n",
        "    temperature=0.1,\n",
        "    do_sample=False,\n",
        "    return_full_text=False\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 3. Load Image and Generate Caption\n",
        "# =====================================================\n",
        "image_path = input(\"Enter the image path of the image you want to analyze: \")\n",
        "\n",
        "try:\n",
        "    image = Image.open(image_path)\n",
        "except:\n",
        "    print(\"âŒ Error: Could not open the image. Please check the path.\")\n",
        "    exit()\n",
        "\n",
        "caption_result = captioner(image)\n",
        "caption = caption_result[0][\"generated_text\"]\n",
        "\n",
        "print(\"\\nðŸ” Image Caption:\")\n",
        "print(caption)\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# =====================================================\n",
        "# 4. Build Prompt for LLM\n",
        "# =====================================================\n",
        "prompt = f\"\"\"\n",
        "You MUST return output in exactly this format:\n",
        "Human: Yes/No\n",
        "Emotion: one of [Happy, Sad, Angry, Neutral, Fear, Surprise, Not Applicable]\n",
        "Reasoning: one sentence explanation\n",
        "\n",
        "Rules:\n",
        "- Exactly ONE output block.\n",
        "- No repetition.\n",
        "- No additional examples.\n",
        "- Do NOT invent any story or context.\n",
        "- Do NOT add information not present in the description.\n",
        "- Do NOT infer emotion from pets, animals, environment, or objects.\n",
        "- Only use explicit emotional cues from the description.\n",
        "- Concerned, worried, nervous â†’ map to Fear.\n",
        "- If no clear emotion matches, use Neutral.\n",
        "- If no human is present, Emotion = Not Applicable.\n",
        "\n",
        "Description:\n",
        "{caption}\n",
        "\"\"\"\n",
        "\n",
        "# =====================================================\n",
        "# 5. Run LLM\n",
        "# =====================================================\n",
        "llm_result = llm(prompt)\n",
        "raw_output = llm_result[0][\"generated_text\"]\n",
        "\n",
        "# =====================================================\n",
        "# 6. Extract First Valid Answer Block\n",
        "# =====================================================\n",
        "pattern = r\"Human:\\s.*?\\nEmotion:\\s.*?\\nReasoning:.*?(?=\\nHuman:|\\Z)\"\n",
        "match = re.search(pattern, raw_output, re.DOTALL)\n",
        "\n",
        "if match:\n",
        "    answer = match.group(0).strip()\n",
        "else:\n",
        "    answer = raw_output.strip()\n",
        "\n",
        "# =====================================================\n",
        "# 7. Parse Fields\n",
        "# =====================================================\n",
        "human_match = re.search(r\"Human:\\s*(Yes|No)\", answer, re.IGNORECASE)\n",
        "emotion_match = re.search(r\"Emotion:\\s*([\\w\\s]+)\", answer, re.IGNORECASE)\n",
        "reasoning_match = re.search(r\"Reasoning:\\s*(.*)\", answer, re.DOTALL | re.IGNORECASE)\n",
        "\n",
        "human = human_match.group(1).capitalize() if human_match else \"Unknown\"\n",
        "emotion = emotion_match.group(1).strip().capitalize() if emotion_match else \"Unknown\"\n",
        "reasoning = reasoning_match.group(1).strip() if reasoning_match else \"\"\n",
        "\n",
        "# =====================================================\n",
        "# 8. Correct Human Detection Logic (Priority Based)\n",
        "# =====================================================\n",
        "caption_lower = caption.lower()\n",
        "\n",
        "human_keywords = [\n",
        "    \"person\", \"people\", \"man\", \"woman\",\n",
        "    \"boy\", \"girl\", \"child\", \"couple\",\n",
        "    \"human\", \"face\"\n",
        "]\n",
        "\n",
        "non_human_keywords = [\n",
        "    \"car\", \"vehicle\", \"building\", \"street\", \"road\",\n",
        "    \"house\", \"tree\", \"nature\", \"landscape\",\n",
        "    \"object\", \"sky\", \"cloud\", \"mountain\"\n",
        "]\n",
        "\n",
        "# Priority:\n",
        "# 1. If human keywords â†’ Human = Yes\n",
        "# 2. Else if non-human keywords â†’ Human = No\n",
        "# 3. Else â†’ Unknown\n",
        "if any(word in caption_lower for word in human_keywords):\n",
        "    human = \"Yes\"\n",
        "elif any(word in caption_lower for word in non_human_keywords):\n",
        "    human = \"No\"\n",
        "else:\n",
        "    human = \"Unknown\"\n",
        "\n",
        "# =====================================================\n",
        "# Hard Override for Non-Human Images\n",
        "# =====================================================\n",
        "if human == \"No\":\n",
        "    emotion = \"Not Applicable\"\n",
        "    reasoning = \"No humans are present in the image, so emotion classification is not applicable.\"\n",
        "\n",
        "# =====================================================\n",
        "# 8.5 Emotion Refinement Layer (Caption-Only + Priority)\n",
        "# =====================================================\n",
        "emotion_map = {\n",
        "    \"Angry\": [\n",
        "        \"angry\", \"anger\", \"furious\", \"annoyed\",\n",
        "        \"shouting\", \"yelling\", \"screaming\",\n",
        "        \"scowl\", \"scowling\", \"glare\", \"glaring\",\n",
        "        \"hostile\", \"aggressive\", \"rage\"\n",
        "    ],\n",
        "    \"Fear\": [\n",
        "        \"fear\", \"afraid\", \"scared\", \"worried\",\n",
        "        \"nervous\", \"anxious\", \"concerned\",\n",
        "        \"terrified\", \"panic\"\n",
        "    ],\n",
        "    \"Sad\": [\n",
        "        \"cry\", \"crying\", \"sad\", \"sadness\",\n",
        "        \"upset\", \"teary\", \"tear\", \"tears\", \"weeping\"\n",
        "    ],\n",
        "    \"Happy\": [\n",
        "        \"smile\", \"smiling\", \"laugh\", \"laughing\",\n",
        "        \"cheerful\", \"joyful\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Priority order: Angry > Fear > Sad > Happy\n",
        "emotion_priority = [\"Angry\", \"Fear\", \"Sad\", \"Happy\"]\n",
        "\n",
        "detected_emotion = None\n",
        "for emotion_label in emotion_priority:\n",
        "    keywords = emotion_map[emotion_label]\n",
        "    if any(word in caption_lower for word in keywords):\n",
        "        detected_emotion = emotion_label\n",
        "        break\n",
        "\n",
        "if human == \"Yes\":\n",
        "    if detected_emotion:\n",
        "        emotion = detected_emotion\n",
        "        reasoning = (\n",
        "            f\"The description contains emotional cues such as {detected_emotion.lower()} related expressions, \"\n",
        "            f\"so the emotion is classified as {emotion}.\"\n",
        "        )\n",
        "    else:\n",
        "        emotion = \"Neutral\"\n",
        "        reasoning = (\n",
        "            \"The description mentions a human, but there are no explicit emotional cues \"\n",
        "            \"such as smiling, crying, anger, or fear. Therefore, the emotion is classified as Neutral.\"\n",
        "        )\n",
        "else:\n",
        "    emotion = \"Not Applicable\"\n",
        "\n",
        "# =====================================================\n",
        "# 9. Prevent Story Hallucination\n",
        "# =====================================================\n",
        "forbidden_words = [\"home\", \"work\", \"school\", \"office\", \"after\", \"day\"]\n",
        "\n",
        "if any(word in reasoning.lower() for word in forbidden_words):\n",
        "    reasoning = (\n",
        "        \"The description contains a human, but no contextual background or life situation \"\n",
        "        \"is provided. The reasoning must rely only on the visual description.\"\n",
        "    )\n",
        "\n",
        "# =====================================================\n",
        "# 10. Final Output\n",
        "# =====================================================\n",
        "final_output = f\"\"\"\n",
        "ðŸ§  Multimodal Reasoning Output\n",
        "-----------------------------\n",
        "Human: {human}\n",
        "Emotion: {emotion}\n",
        "Reasoning: {reasoning}\n",
        "\"\"\"\n",
        "\n",
        "print(final_output)\n"
      ],
      "metadata": {
        "id": "_PFXM9ROkfKk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}