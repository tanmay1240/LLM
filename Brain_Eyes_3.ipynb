{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4kJeLtiNVALGqaiD8bQT8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanmay1240/LLM/blob/main/Brain_Eyes_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multimodal Emotion Detection System â€“ Version 2\n",
        "### Emotion recognition using Vision + Caption + VQA + FER fusion\n"
      ],
      "metadata": {
        "id": "829Z7np5zUAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers torch\n"
      ],
      "metadata": {
        "id": "-0IGbAQYKRly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from PIL import Image\n",
        "import re\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import torch\n",
        "\n",
        "# =====================================================\n",
        "# 1. Load Vision Model (BLIP)\n",
        "# =====================================================\n",
        "captioner = pipeline(\n",
        "    \"image-to-text\",\n",
        "    model=\"Salesforce/blip-image-captioning-large\"\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 1.5 Load VQA Model (Emotion Probe)\n",
        "# =====================================================\n",
        "vqa = pipeline(\n",
        "    \"visual-question-answering\",\n",
        "    model=\"Salesforce/blip-vqa-base\"\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 1.6 Load CLIP Model\n",
        "# =====================================================\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# =====================================================\n",
        "# 2. Load LLM\n",
        "# =====================================================\n",
        "llm = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "    max_new_tokens=120,\n",
        "    temperature=0.1,\n",
        "    do_sample=False,\n",
        "    return_full_text=False\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 3. Load Image and Generate Caption\n",
        "# =====================================================\n",
        "image_path = input(\"Enter the image path of the image you want to analyze: \")\n",
        "\n",
        "try:\n",
        "    image = Image.open(image_path)\n",
        "except:\n",
        "    print(\"âŒ Error: Could not open the image. Please check the path.\")\n",
        "    exit()\n",
        "\n",
        "caption_result = captioner(image)\n",
        "caption = caption_result[0][\"generated_text\"]\n",
        "\n",
        "print(\"\\nðŸ” Image Caption:\")\n",
        "print(caption)\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# -------------------------------\n",
        "# VQA Emotion Hint (Moved here after image is loaded)\n",
        "# -------------------------------\n",
        "vqa_result = vqa(\n",
        "    image=image,\n",
        "    question=\"What emotion is the person showing?\"\n",
        ")\n",
        "\n",
        "vqa_emotion = vqa_result[0][\"answer\"].lower()\n",
        "\n",
        "print(\"\\nðŸŽ¯ VQA Emotion Hint:\")\n",
        "print(vqa_emotion)\n",
        "\n",
        "# -------------------------------\n",
        "# CLIP Emotion Prediction\n",
        "# -------------------------------\n",
        "clip_prompts = [\n",
        "    \"a person who is happy\",\n",
        "    \"a person who is sad\",\n",
        "    \"a person who is angry\",\n",
        "    \"a person who is afraid\",\n",
        "    \"a person with a neutral expression\"\n",
        "]\n",
        "\n",
        "inputs = clip_processor(\n",
        "    text=clip_prompts,\n",
        "    images=image,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = clip_model(**inputs)\n",
        "    logits_per_image = outputs.logits_per_image\n",
        "    probs = logits_per_image.softmax(dim=1)\n",
        "\n",
        "clip_index = probs.argmax().item()\n",
        "clip_emotion = clip_prompts[clip_index].split(\"who is \")[-1].replace(\"a person \", \"\").strip()\n",
        "\n",
        "# Normalize\n",
        "clip_emotion = clip_emotion.replace(\"with a \", \"\").replace(\" expression\", \"\").capitalize()\n",
        "\n",
        "print(\"\\nðŸŽ¯ CLIP Emotion Prediction:\")\n",
        "print(clip_emotion)\n",
        "\n",
        "# =====================================================\n",
        "# 4. Build Prompt for LLM\n",
        "# =====================================================\n",
        "prompt = f\"\"\"\n",
        "You MUST return output in exactly this format:\n",
        "Human: Yes/No\n",
        "Emotion: one of [Happy, Sad, Angry, Neutral, Fear, Surprise, Not Applicable]\n",
        "Reasoning: one sentence explanation\n",
        "\n",
        "Rules:\n",
        "- Exactly ONE output block.\n",
        "- No repetition.\n",
        "- No additional examples.\n",
        "- Do NOT invent any story or context.\n",
        "- Do NOT add information not present in the description.\n",
        "- Do NOT infer emotion from pets, animals, environment, or objects.\n",
        "- Only use explicit emotional cues from the description.\n",
        "- Concerned, worried, nervous â†’ map to Fear.\n",
        "- If no clear emotion matches, use Neutral.\n",
        "- If no human is present, Emotion = Not Applicable.\n",
        "\n",
        "Description:\n",
        "{caption}\n",
        "\"\"\"\n",
        "\n",
        "# =====================================================\n",
        "# 5. Run LLM\n",
        "# =====================================================\n",
        "llm_result = llm(prompt)\n",
        "raw_output = llm_result[0][\"generated_text\"]\n",
        "\n",
        "# =====================================================\n",
        "# 6. Extract First Valid Answer Block\n",
        "# =====================================================\n",
        "pattern = r\"Human:\\s.*?\\nEmotion:\\s.*?\\nReasoning:.*?(?=\\nHuman:|\\Z)\"\n",
        "match = re.search(pattern, raw_output, re.DOTALL)\n",
        "\n",
        "if match:\n",
        "    answer = match.group(0).strip()\n",
        "else:\n",
        "    answer = raw_output.strip()\n",
        "\n",
        "# =====================================================\n",
        "# 7. Parse Fields\n",
        "# =====================================================\n",
        "human_match = re.search(r\"Human:\\s*(Yes|No)\", answer, re.IGNORECASE)\n",
        "emotion_match = re.search(r\"Emotion:\\s*([\\w\\s]+)\", answer, re.IGNORECASE)\n",
        "reasoning_match = re.search(r\"Reasoning:\\s*(.*)\", answer, re.DOTALL | re.IGNORECASE)\n",
        "\n",
        "human = human_match.group(1).capitalize() if human_match else \"Unknown\"\n",
        "emotion = emotion_match.group(1).strip().capitalize() if emotion_match else \"Unknown\"\n",
        "reasoning = reasoning_match.group(1).strip() if reasoning_match else \"\"\n",
        "\n",
        "# =====================================================\n",
        "# 8. Correct Human Detection Logic (Priority Based)\n",
        "# =====================================================\n",
        "caption_lower = caption.lower()\n",
        "\n",
        "human_keywords = [\n",
        "    \"person\", \"people\", \"man\", \"woman\",\n",
        "    \"boy\", \"girl\", \"child\", \"couple\",\n",
        "    \"human\", \"face\"\n",
        "]\n",
        "\n",
        "non_human_keywords = [\n",
        "    \"car\", \"vehicle\", \"building\", \"street\", \"road\",\n",
        "    \"house\", \"tree\", \"nature\", \"landscape\",\n",
        "    \"object\", \"sky\", \"cloud\", \"mountain\"\n",
        "]\n",
        "\n",
        "# Priority:\n",
        "# 1. If human keywords â†’ Human = Yes\n",
        "# 2. Else if non-human keywords â†’ Human = No\n",
        "# 3. Else â†’ Unknown\n",
        "if any(word in caption_lower for word in human_keywords):\n",
        "    human = \"Yes\"\n",
        "elif any(word in caption_lower for word in non_human_keywords):\n",
        "    human = \"No\"\n",
        "else:\n",
        "    human = \"Unknown\"\n",
        "\n",
        "# =====================================================\n",
        "# Hard Override for Non-Human Images\n",
        "# =====================================================\n",
        "if human == \"No\":\n",
        "    emotion = \"Not Applicable\"\n",
        "    reasoning = \"No humans are present in the image, so emotion classification is not applicable.\"\n",
        "\n",
        "# =====================================================\n",
        "# 8.5 Emotion Refinement Layer (Caption-Only + Priority)\n",
        "# =====================================================\n",
        "emotion_map = {\n",
        "    \"Angry\": [\n",
        "        \"angry\", \"anger\", \"furious\", \"annoyed\",\n",
        "        \"shouting\", \"yelling\", \"screaming\",\n",
        "        \"scowl\", \"scowling\", \"glare\", \"glaring\",\n",
        "        \"hostile\", \"aggressive\", \"rage\"\n",
        "    ],\n",
        "    \"Fear\": [\n",
        "        \"fear\", \"afraid\", \"scared\", \"worried\",\n",
        "        \"nervous\", \"anxious\", \"concerned\",\n",
        "        \"terrified\", \"panic\"\n",
        "    ],\n",
        "    \"Sad\": [\n",
        "        \"cry\", \"crying\", \"sad\", \"sadness\",\n",
        "        \"upset\", \"teary\", \"tear\", \"tears\", \"weeping\"\n",
        "    ],\n",
        "    \"Happy\": [\n",
        "        \"smile\", \"smiling\", \"laugh\", \"laughing\",\n",
        "        \"cheerful\", \"joyful\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Priority order: Angry > Fear > Sad > Happy\n",
        "emotion_priority = [\"Angry\", \"Fear\", \"Sad\", \"Happy\"]\n",
        "\n",
        "detected_emotion = None\n",
        "for emotion_label in emotion_priority:\n",
        "    keywords = emotion_map[emotion_label]\n",
        "    if any(word in caption_lower for word in keywords):\n",
        "        detected_emotion = emotion_label\n",
        "        break\n",
        "\n",
        "# =====================================================\n",
        "# Emotion Fusion Logic (Caption â†’ VQA â†’ CLIP)\n",
        "# =====================================================\n",
        "\n",
        "valid_emotions = [\"Happy\", \"Sad\", \"Angry\", \"Fear\", \"Neutral\"]\n",
        "\n",
        "if human == \"Yes\":\n",
        "    # 1. Caption-based emotion (highest priority)\n",
        "    if detected_emotion:\n",
        "        emotion = detected_emotion\n",
        "        reasoning = (\n",
        "            f\"The description contains explicit emotional cues related to {emotion.lower()}, \"\n",
        "            f\"so the emotion is classified as {emotion}.\"\n",
        "        )\n",
        "\n",
        "    # 2. VQA-based emotion\n",
        "    elif vqa_emotion in [e.lower() for e in valid_emotions]:\n",
        "        emotion = vqa_emotion.capitalize()\n",
        "        if emotion == \"Scared\":\n",
        "            emotion = \"Fear\"\n",
        "\n",
        "        reasoning = (\n",
        "            f\"The visual question answering model suggests the emotion '{emotion}', \"\n",
        "            f\"which is used when the caption does not provide explicit emotional cues.\"\n",
        "        )\n",
        "\n",
        "    # 3. CLIP-based emotion\n",
        "    elif clip_emotion in valid_emotions:\n",
        "        emotion = clip_emotion\n",
        "        reasoning = (\n",
        "            f\"The CLIP model shows highest similarity between the image and the concept '{emotion}', \"\n",
        "            f\"so the emotion is classified as {emotion}.\"\n",
        "        )\n",
        "\n",
        "    # 4. Fallback\n",
        "    else:\n",
        "        emotion = \"Neutral\"\n",
        "        reasoning = (\n",
        "            \"Neither the caption, the visual question answering model, nor the CLIP model \"\n",
        "            \"provides strong emotional evidence, so the emotion is classified as Neutral.\"\n",
        "        )\n",
        "else:\n",
        "    emotion = \"Not Applicable\"\n",
        "    reasoning = \"No humans are present in the image, so emotion classification is not applicable.\"\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# 9. Prevent Story Hallucination\n",
        "# =====================================================\n",
        "forbidden_words = [\"home\", \"work\", \"school\", \"office\", \"after\", \"day\"]\n",
        "\n",
        "if any(word in reasoning.lower() for word in forbidden_words):\n",
        "    reasoning = (\n",
        "        \"The description contains a human, but no contextual background or life situation \"\n",
        "        \"is provided. The reasoning must rely only on the visual description.\"\n",
        "    )\n",
        "\n",
        "# =====================================================\n",
        "# 10. Final Output\n",
        "# =====================================================\n",
        "final_output = f\"\"\"\n",
        "ðŸ§  Multimodal Reasoning Output\n",
        "-----------------------------\n",
        "Human: {human}\n",
        "Emotion: {emotion}\n",
        "Reasoning: {reasoning}\n",
        "\"\"\"\n",
        "\n",
        "print(final_output)\n"
      ],
      "metadata": {
        "id": "CXaK-9TDKVnO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}